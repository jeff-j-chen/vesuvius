{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vesuvius\n",
    "from vesuvius import Volume\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InkVolumeDataset(Dataset):\n",
    "    def __init__(self, volume, labels, tile_size, depth):\n",
    "        \"\"\"\n",
    "        volume: [D, H, W] - 3D volume of grayscale slices\n",
    "        labels: [H, W] - 2D binary mask shared across depth\n",
    "        tile_size: size of each 2D tile (height and width)\n",
    "        depth: number of slices to stack per sample\n",
    "        \"\"\"\n",
    "        self.volume = volume\n",
    "        self.labels = labels\n",
    "        self.tile_size = tile_size\n",
    "        self.depth = depth\n",
    "        self.D, self.H, self.W = volume.shape\n",
    "\n",
    "        self.blocks = []\n",
    "        for d in range(0, self.D - depth + 1, int(depth//2)):\n",
    "            for y in range(0, self.H - tile_size + 1, tile_size):\n",
    "                for x in range(0, self.W - tile_size + 1, tile_size):\n",
    "                    label_tile = labels[y:y+tile_size, x:x+tile_size]\n",
    "                    if label_tile.shape == (tile_size, tile_size):\n",
    "                        self.blocks.append((d, y, x))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d, y, x = self.blocks[idx]\n",
    "        \n",
    "        block = self.volume[d:d+self.depth, y:y+self.tile_size, x:x+self.tile_size]\n",
    "        label_tile = self.labels[y:y+self.tile_size, x:x+self.tile_size]\n",
    "\n",
    "        # Convert to tensor and ensure proper normalization\n",
    "        # Don't divide by 255 again if already normalized\n",
    "        block = torch.tensor(block, dtype=torch.float32)\n",
    "        \n",
    "        # Add channel dimension: [D, H, W] -> [1, D, H, W]\n",
    "        block = block.unsqueeze(0)\n",
    "\n",
    "        # Binary label: 1 if any ink present (more robust checking)\n",
    "        has_ink = np.any(label_tile > 0.5)  # More robust than == 1.0\n",
    "        label = torch.tensor([float(has_ink)], dtype=torch.float32)\n",
    "\n",
    "        return block, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InkDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InkDetector, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3),\n",
    "\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d(1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: Counter({0: 155910, 1: 55770})\n",
      "Using pos_weight: 2.80\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "segment_id = 20230827161847\n",
    "segment = Volume(segment_id, normalize=True)\n",
    "\n",
    "# Extract volume and labels\n",
    "volume = segment[:64, 200:5600, 1000:4600]\n",
    "labels = segment.inklabel[200:5600, 1000:4600] / 255.0\n",
    "\n",
    "# Data setup\n",
    "tile_size = 32\n",
    "depth = 8\n",
    "split_x = int(volume.shape[2] * 0.75)\n",
    "\n",
    "train_volume = volume[:, :, :split_x]\n",
    "train_labels = labels[:, :split_x]\n",
    "valid_volume = volume[:, :, split_x:]\n",
    "valid_labels = labels[:, split_x:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InkVolumeDataset(train_volume, train_labels, tile_size=tile_size, depth=depth)\n",
    "valid_dataset = InkVolumeDataset(valid_volume, valid_labels, tile_size=tile_size, depth=depth)\n",
    "\n",
    "# Check label distribution\n",
    "all_labels = [int(label.item()) for _, label in train_dataset]\n",
    "label_counts = Counter(all_labels)\n",
    "print(f\"Label distribution: {label_counts}\")\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "pos_weight = None\n",
    "if label_counts[0] > 0 and label_counts[1] > 0:\n",
    "    pos_weight = torch.tensor([label_counts[0] / label_counts[1]])\n",
    "    print(f\"Using pos_weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8  # Reduced batch size for debugging\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Acc: 0.4547 | Val Acc: 0.7708 | Train Loss: 1.0212 | Val Loss: 0.9777\n",
      "Epoch 2/20 | Train Acc: 0.4941 | Val Acc: 0.7708 | Train Loss: 1.0211 | Val Loss: 0.9768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 28\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Fixed prediction logic\u001b[39;00m\n\u001b[1;32m     31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msigmoid(outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Apply sigmoid first\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = InkDetector().to(device)\n",
    "\n",
    "# Use weighted loss for imbalanced data\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device) if pos_weight is not None else None)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for batch_images, batch_labels in train_loader:\n",
    "        # batch_images already has shape [B, 1, D, H, W] from dataset\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_labels = batch_labels.to(device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_images)\n",
    "\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Fixed prediction logic\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()  # Apply sigmoid first\n",
    "        train_correct += (predicted == batch_labels).sum().item()\n",
    "        train_total += batch_labels.size(0)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).view(-1, 1)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            \n",
    "            # Fixed prediction logic\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"Train Acc: {train_correct/train_total:.4f} | Val Acc: {val_correct/val_total:.4f} | \"\n",
    "        f\"Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(valid_loader):.4f}\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_stitched_volumes_depth_blocks(model, train_volume, train_labels, valid_volume, valid_labels, \n",
    "                                          tile_size=16, depth=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Validate model on both train and validation volumes across multiple depth blocks, \n",
    "    then stitch them together for visualization. Creates separate visualizations for each depth block.\n",
    "    Assumes train_volume and valid_volume were split horizontally (along width axis).\n",
    "    \n",
    "    Number of depth blocks is automatically calculated based on volume depth and depth parameter\n",
    "    to match the dataset creation logic (non-overlapping blocks).\n",
    "    \"\"\"\n",
    "    \n",
    "    def process_volume_depth_block(volume, labels, volume_name, depth_start, depth_end):\n",
    "        \"\"\"Helper function to process a single volume at a specific depth range\"\"\"\n",
    "        model.eval()\n",
    "        D, H, W = volume.shape\n",
    "        \n",
    "        prediction_map = np.zeros((H, W), dtype=np.float32)\n",
    "        count_map = np.zeros((H, W), dtype=np.float32)\n",
    "        \n",
    "        # Create list of all tile coordinates\n",
    "        tile_coords = []\n",
    "        for y in range(0, H - tile_size + 1, tile_size):\n",
    "            for x in range(0, W - tile_size + 1, tile_size):\n",
    "                tile_coords.append((y, x))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process tiles with tqdm progress bar\n",
    "            for y, x in tqdm(tile_coords, desc=f\"Processing {volume_name} volume (depth {depth_start}-{depth_end-1})\"):\n",
    "                # Extract block from the specified depth range\n",
    "                block = volume[depth_start:depth_end, y:y+tile_size, x:x+tile_size]\n",
    "                \n",
    "                if block.shape == (depth, tile_size, tile_size):\n",
    "                    block_tensor = torch.from_numpy(block).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    logits = model(block_tensor)\n",
    "                    pred = torch.sigmoid(logits).item()\n",
    "                    \n",
    "                    prediction_map[y:y+tile_size, x:x+tile_size] += pred\n",
    "                    count_map[y:y+tile_size, x:x+tile_size] += 1\n",
    "        \n",
    "        # Normalize predictions\n",
    "        prediction_map = np.divide(prediction_map, count_map, where=count_map>0)\n",
    "        return prediction_map\n",
    "    \n",
    "    # Calculate number of depth blocks to match dataset creation logic\n",
    "    D = train_volume.shape[0]\n",
    "    num_depth_blocks = (D - depth + 1) // depth\n",
    "    \n",
    "    print(f\"Volume depth: {D}, Block depth: {depth}, Number of blocks: {num_depth_blocks}\")\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each depth block (matching dataset creation logic)\n",
    "    for block_idx in range(num_depth_blocks):\n",
    "        depth_start = block_idx * depth\n",
    "        depth_end = depth_start + depth\n",
    "        \n",
    "        print(f\"\\n=== Processing Depth Block {block_idx + 1}/{num_depth_blocks} (slices {depth_start}-{depth_end-1}) ===\")\n",
    "        \n",
    "        # Process both volumes for this depth block\n",
    "        train_predictions = process_volume_depth_block(train_volume, train_labels, \"training\", depth_start, depth_end)\n",
    "        valid_predictions = process_volume_depth_block(valid_volume, valid_labels, \"validation\", depth_start, depth_end)\n",
    "        \n",
    "        # Stitch everything back together horizontally\n",
    "        # Use the middle slice of the current depth block for visualization\n",
    "        middle_slice_idx = depth_start + depth // 2\n",
    "        full_volume_slice = np.concatenate([train_volume[middle_slice_idx], valid_volume[middle_slice_idx]], axis=1)\n",
    "        full_labels = np.concatenate([train_labels, valid_labels], axis=1)\n",
    "        full_predictions = np.concatenate([train_predictions, valid_predictions], axis=1)\n",
    "        \n",
    "        # Create visualization for this depth block\n",
    "        plt.figure(figsize=(24, 6))\n",
    "        \n",
    "        # Original slice\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(full_volume_slice, cmap='gray')\n",
    "        plt.title(f'Full Volume (Slice {middle_slice_idx})\\nDepth Block {block_idx + 1} ({depth_start}-{depth_end-1})\\nTrain | Valid')\n",
    "        plt.axvline(x=train_volume.shape[2]-0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(full_labels, cmap='binary')\n",
    "        plt.title(f'Ground Truth Labels\\nDepth Block {block_idx + 1}\\nTrain | Valid')\n",
    "        plt.axvline(x=train_labels.shape[1]-0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Predictions\n",
    "        plt.subplot(1, 4, 3)\n",
    "        img = plt.imshow(full_predictions, cmap='pink', vmin=0, vmax=1)\n",
    "        plt.colorbar(img, fraction=0.046, pad=0.04)\n",
    "        plt.title(f'Model Predictions\\nDepth Block {block_idx + 1}\\nTrain | Valid')\n",
    "        plt.axvline(x=train_predictions.shape[1]-0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(full_predictions, cmap='inferno', vmin=0, vmax=1)\n",
    "        \n",
    "        # Create overlay for ground truth\n",
    "        label_overlay = np.zeros((*full_labels.shape, 4))  # RGBA\n",
    "        label_overlay[full_labels > 0.5] = [1, 1, 1, 0.1]  # White with transparency\n",
    "        plt.imshow(label_overlay)\n",
    "        \n",
    "        plt.title(f'Predictions + Ground Truth\\nDepth Block {block_idx + 1}\\nTrain | Valid\\n(White = True Labels)')\n",
    "        plt.axvline(x=train_predictions.shape[1]-0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'depth_block': block_idx + 1,\n",
    "            'depth_range': (depth_start, depth_end-1),\n",
    "            'train_predictions': train_predictions,\n",
    "            'valid_predictions': valid_predictions,\n",
    "            'full_predictions': full_predictions,\n",
    "            'full_labels': full_labels,\n",
    "            'full_volume_slice': full_volume_slice\n",
    "        }\n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Print overall summary\n",
    "    print(f\"\\n=== OVERALL SUMMARY ===\")\n",
    "    print(f\"Processed {num_depth_blocks} depth blocks of {depth} slices each\")\n",
    "    print(f\"Ground truth ink pixels: {(full_labels > 0.5).sum()} pixels\")\n",
    "    print(f\"Train region width: {train_volume.shape[2]}\")\n",
    "    print(f\"Valid region width: {valid_volume.shape[2]}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validate_stitched_volumes_depth_blocks(model, train_volume, train_labels, valid_volume, valid_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
